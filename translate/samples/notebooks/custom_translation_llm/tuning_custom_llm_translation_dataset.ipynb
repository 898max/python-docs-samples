{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "HkWlLSwxpgBd",
        "oHrrDiHwtm8y",
        "2mzgIOtOqJAe",
        "-guLLW3wV8Ub"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "metadata": {
        "id": "A_SGDORxbPGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/python-docs-samples/blob/cloud_translation_docs/translate/samples/notebooks/custom_translation_llm/tuning_custom_llm_translation_dataset.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fpython-docs-samples%2Fcloud_translation_docs%2Ftranslate%2Fsamples%2Fnotebooks%2Fcustom_translation_llm%2Ftuning_custom_llm_translation_dataset.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/python-docs-samples/tree/cloud_translation_docs/translate/samples/notebooks/custom_translation_llm/tuning_custom_llm_translation_dataset.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>"
      ],
      "metadata": {
        "id": "3OwDULtszPR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Colab example initiates a tuning job on base model `translation-llm-002` with provided [translation dataset](https://cloud.google.com/translate/docs/advanced/automl-quickstart?_gl=1*1uxuas*_up*MQ..&gclid=Cj0KCQiAvP-6BhDyARIsAJ3uv7bZUmDC7sTZ4d5N-08d2vtOXi9smnSTsI0YDQHK49fxYKwy68eAAtMaAixREALw_wcB&gclsrc=aw.ds) (legacy datasets are not supported)."
      ],
      "metadata": {
        "id": "5EC94Xly2pmP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prerequisites"
      ],
      "metadata": {
        "id": "H6RvoTU-zSvP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Enable `Vertex AI` API in `APIs & services` page.\n",
        "*   It's recommended to use Colab for running the script, as it best supports the authentication process and Cloud CLI."
      ],
      "metadata": {
        "id": "L-tS_wwgzSvU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Authentication"
      ],
      "metadata": {
        "id": "pMpx_VgHokrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = \"my-project\"  # @param {type:\"string\"}"
      ],
      "metadata": {
        "id": "NCgN8AQhYRdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud auth application-default login\n",
        "!gcloud auth application-default set-quota-project {PROJECT_ID}"
      ],
      "metadata": {
        "id": "P-JukDsyTyad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Input parameters"
      ],
      "metadata": {
        "id": "Q6xZ3eVVpDhJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To quick start: First import a native dataset at [AutoML](https://console.cloud.google.com/translation/datasets), and get dataset id under the display name. Fill in the required parameters.\n",
        "\n",
        "By default, the model name to be used for translate text requests will be returned after the tuning finishes. For your reference, the tuning will take less than 20 minutes for a dataset with 10k training examples."
      ],
      "metadata": {
        "id": "W5d3FdGv2Pli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory to save converted dataset.\n",
        "GCS_EXPORT_PATH = 'gs://my_bucket/dir' # @param {type:\"string\"}\n",
        "\n",
        "# Only native datasets are supported.\n",
        "DATASET_ID = '123abc' # @param {type:\"string\"}\n",
        "\n",
        "# Model display name on Vertex AI Online Prediction page.\n",
        "TUNED_MODEL_DISPLAY_NAME = 'translation-llm-test' # @param {type:\"string\"}\n",
        "\n",
        "# Set sample size. Set to \"-1\" to use all examples.\n",
        "TRAIN_DATASET_SAMPLE_SIZE = -1 # @param {type:\"integer\"}\n",
        "\n",
        "# Validation size limit is 1000.\n",
        "VALIDATION_DATASET_SAMPLE_SIZE = 250 # @param {type:\"integer\"}\n"
      ],
      "metadata": {
        "id": "cs_GIMWVnAoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Helper functions"
      ],
      "metadata": {
        "id": "HkWlLSwxpgBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# only us-central1 is supported for now\n",
        "LOCATION = 'us-central1'\n",
        "\n",
        "language_map = {\n",
        "    'en' : 'English',\n",
        "    'es' : 'Spanish',\n",
        "    'fr' : 'French',\n",
        "    'de' : 'German',\n",
        "    'it' : 'Italian',\n",
        "    'pt' : 'Portuguese',\n",
        "    'zh' : 'Chinese',\n",
        "    'ja' : 'Japanese',\n",
        "    'ko' : 'Korean',\n",
        "    'ar' : 'Arabic',\n",
        "    'hi' : 'Hindi',\n",
        "    'ru' : 'Russian',\n",
        "}"
      ],
      "metadata": {
        "id": "3EqyZ4ZDfpTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import json\n",
        "import glob\n",
        "import os\n",
        "import time\n",
        "\n",
        "from google.cloud import translate_v3\n",
        "from google.cloud import storage\n",
        "\n",
        "import vertexai\n",
        "from vertexai.tuning import sft\n",
        "\n",
        "\n",
        "# Creates single json tuning input data\n",
        "def convert_line_to_jsonl(source_language, target_language, source_sentence, target_sentence):\n",
        "  return json.dumps({\n",
        "      \"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": source_language + \": \" + source_sentence + \" \" + target_language + \": \"}]},\n",
        "       {\"role\": \"model\", \"parts\": [{\"text\": target_sentence}]}]}, ensure_ascii=False)\n",
        "\n",
        "\n",
        "# Checking dataset exists and extract language pairs\n",
        "def check_dataset(project_id, location, dataset_id):\n",
        "  translation_client = translate_v3.TranslationServiceClient()\n",
        "  request = translate_v3.GetDatasetRequest(\n",
        "    name=f\"projects/{project_id}/locations/{location}/datasets/{dataset_id}\",\n",
        "  )\n",
        "  try:\n",
        "    response = translation_client.get_dataset(request=request)\n",
        "    print(response)\n",
        "    if response.source_language_code not in language_map or response.target_language_code not in language_map:\n",
        "      raise ValueError(\"Invalid language code\")\n",
        "    return response.source_language_code, response.target_language_code\n",
        "  except Exception as e:\n",
        "    raise ValueError(f\"Error getting dataset: {e}\")\n",
        "\n",
        "\n",
        "# Export dataset to gcs directory\n",
        "def export_data(project_id, location, dataset_id, gcs_export_path):\n",
        "  translation_client = translate_v3.TranslationServiceClient()\n",
        "\n",
        "  # Initialize request argument(s)\n",
        "  output_config = translate_v3.DatasetOutputConfig()\n",
        "  output_config.gcs_destination.output_uri_prefix = gcs_export_path\n",
        "\n",
        "  export_request = translate_v3.ExportDataRequest(\n",
        "    dataset=f\"projects/{project_id}/locations/{location}/datasets/{dataset_id}\",\n",
        "    output_config=output_config,\n",
        "  )\n",
        "\n",
        "  # Make the request\n",
        "  response = translation_client.export_data(request=export_request)\n",
        "\n",
        "  print(\"Waiting for operation to complete...\")\n",
        "\n",
        "  while not response.done():\n",
        "    time.sleep(5)\n",
        "\n",
        "  if response.metadata.error.message:\n",
        "    print(\"Dataset exported failed.\")\n",
        "    print(response.metadata.error.message)\n",
        "    return \"\"\n",
        "  else:\n",
        "    print(\"Dataset exported successfully.\")\n",
        "    operation_short_name = response.operation.name.rsplit('/', 1)[-1]\n",
        "    exported_bucket = gcs_export_path + '/exported_' + dataset_id + '_' + operation_short_name\n",
        "    print(exported_bucket)\n",
        "    return exported_bucket\n",
        "\n",
        "\n",
        "# Format conversion function as part of the AutoML export workflow.\n",
        "def convert_exported_files(colab_path, source_language_code, target_language_code, train_dataset_sample_size, validation_dataset_sample_size):\n",
        "  train_file_list = glob.glob(colab_path + '/train*')\n",
        "  validation_file_list = glob.glob(colab_path + '/validation*')\n",
        "  train_jsonl = os.path.join(colab_path, \"train.jsonl\")\n",
        "  validation_jsonl = os.path.join(colab_path, \"validation.jsonl\")\n",
        "\n",
        "  with open(train_jsonl, 'w', encoding='utf-8') as outfile:\n",
        "    count = 0\n",
        "    for train_file in train_file_list:\n",
        "      with open(train_file, 'r', encoding='utf-8') as infile:\n",
        "        reader = csv.reader(infile, delimiter='\\t')\n",
        "        for row in reader:\n",
        "          message = convert_line_to_jsonl(language_map[source_language_code], language_map[target_language_code], row[0], row[1])\n",
        "          outfile.write(message)\n",
        "          outfile.write('\\n')\n",
        "          count += 1\n",
        "          if count == train_dataset_sample_size:\n",
        "            break\n",
        "      if count == train_dataset_sample_size:\n",
        "        break\n",
        "\n",
        "  with open(validation_jsonl, 'w', encoding='utf-8') as outfile:\n",
        "    count = 0\n",
        "    for validation_file in validation_file_list:\n",
        "      with open(validation_file, 'r', encoding='utf-8') as infile:\n",
        "        reader = csv.reader(infile, delimiter='\\t')\n",
        "        for row in reader:\n",
        "          message = convert_line_to_jsonl(language_map[source_language_code], language_map[target_language_code], row[0], row[1])\n",
        "          outfile.write(message)\n",
        "          outfile.write('\\n')\n",
        "          count += 1\n",
        "          if count == validation_dataset_sample_size:\n",
        "            break\n",
        "      if count == validation_dataset_sample_size:\n",
        "        break\n",
        "\n",
        "  print(\"File conversion completed.\")\n",
        "\n",
        "\n",
        "# Initiates model training\n",
        "def train_model(train_dataset_path, validation_dataset_path, display_name):\n",
        "  vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "  sft_tuning_job = sft.train(\n",
        "    source_model=\"translation-llm-002\",\n",
        "    train_dataset=train_dataset_path,\n",
        "    validation_dataset=validation_dataset_path,\n",
        "    tuned_model_display_name=display_name,\n",
        "  )\n",
        "\n",
        "  # Polling for job completion\n",
        "  while not sft_tuning_job.has_ended:\n",
        "    time.sleep(60)\n",
        "    sft_tuning_job.refresh()\n",
        "\n",
        "  endpoint_short_name = sft_tuning_job.tuned_model_endpoint_name.rsplit('/', 1)[-1]\n",
        "  custom_model_name = f\"projects/{PROJECT_ID}/locations/{LOCATION}/models/translation-llm-custom/{endpoint_short_name}\"\n",
        "\n",
        "  print(\"Model: \", custom_model_name)\n",
        "  return custom_model_name\n"
      ],
      "metadata": {
        "id": "dTMTU9SYd3dE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export dataset from AutoML"
      ],
      "metadata": {
        "id": "oHrrDiHwtm8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GCS_EXPORT_PATH = GCS_EXPORT_PATH.rstrip('/')\n",
        "SOURCE_LANGUAGE_CODE, TARGET_LANGUAGE_CODE = check_dataset(PROJECT_ID, LOCATION, DATASET_ID)\n",
        "exported_bucket = export_data(PROJECT_ID, LOCATION, DATASET_ID, GCS_EXPORT_PATH)"
      ],
      "metadata": {
        "id": "WjW6GkxvyEh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil cp -r {exported_bucket} '/content/'"
      ],
      "metadata": {
        "id": "PJe8kxrK5p7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Optional: Delete exported files in gcs bucket."
      ],
      "metadata": {
        "id": "KvfYGuTWj318"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil rm -r {exported_bucket}"
      ],
      "metadata": {
        "id": "_zKufnn0jXPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dataset Format Conversion"
      ],
      "metadata": {
        "id": "2mzgIOtOqJAe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step convers data to `.jsonl` format for tuning."
      ],
      "metadata": {
        "id": "t-fj4DU6Bsv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "colab_path = os.path.join('/content/', exported_bucket.rsplit('/', 1)[-1])\n",
        "convert_exported_files(colab_path, SOURCE_LANGUAGE_CODE, TARGET_LANGUAGE_CODE, TRAIN_DATASET_SAMPLE_SIZE, VALIDATION_DATASET_SAMPLE_SIZE)\n",
        "train_jsonl = os.path.join(colab_path, 'train.jsonl')\n",
        "validation_jsonl = os.path.join(colab_path, 'validation.jsonl')\n",
        "train_dataset_path = GCS_EXPORT_PATH + '/' + DATASET_ID + '_train.jsonl'\n",
        "validation_dataset_path = GCS_EXPORT_PATH + '/' + DATASET_ID + '_validation.jsonl'"
      ],
      "metadata": {
        "id": "5FBPjzlDB376"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil cp {train_jsonl} {train_dataset_path}\n",
        "!gsutil cp {validation_jsonl} {validation_dataset_path}"
      ],
      "metadata": {
        "id": "OUy_iz3yCSGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Optional: Remove dataset copied to Colab"
      ],
      "metadata": {
        "id": "tugoBkAv273E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf {colab_path}"
      ],
      "metadata": {
        "id": "jQns2hUb3CI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initiate Vertex Tuning Request"
      ],
      "metadata": {
        "id": "-guLLW3wV8Ub"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After tuning is done, the translation model name will be returned to be used for translation requests."
      ],
      "metadata": {
        "id": "wZVCR7wiB6Dc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "custom_model_name = train_model(train_dataset_path, validation_dataset_path, TUNED_MODEL_DISPLAY_NAME)"
      ],
      "metadata": {
        "id": "ARvHS1BLWIHX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}